{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Clear GPU memory\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "JGmLGopZyMNX"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPi6VwE8yWBS",
        "outputId": "6219bf9f-9e3f-4a34-aebb-0a7fe8fceb78"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri May  5 02:43:49 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   53C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNrEJW1jTomp",
        "outputId": "31b3e214-142b-4c42-ade9-8224d9655092"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.4.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.14.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset('allenai/scirepeval','search', split='evaluation')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDnzPjkST5b-",
        "outputId": "1d9a80be-3f3c-4cd0-9f6f-c54211eac216"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset scirepeval (/root/.cache/huggingface/datasets/allenai___scirepeval/search/1.1.0/05b6b341c1750891a5539df5b3eb892babd18e8f299bfaba4735b87e20f6cf83)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "papers = dataset['candidates']"
      ],
      "metadata": {
        "id": "-1_Aj8FnT7EZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "papers_dict ={}\n",
        "ground_truth = []\n",
        "for i in range(len(papers)):\n",
        "  temp = []\n",
        "  for j in range(10):\n",
        "    papers_dict[papers[i][j]['corpus_id']] = papers[i][j]['title']+ papers[i][j]['abstract']\n",
        "    temp.append(papers[i][j]['corpus_id'])\n",
        "  ground_truth.append(temp)"
      ],
      "metadata": {
        "id": "eujvMtqvT_Ng"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_dataset = [{'id': key, 'abstract': value} for key, value in papers_dict.items()]"
      ],
      "metadata": {
        "id": "IqBmVwA9UWRB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3Ui8Y_mUZIW",
        "outputId": "741e4e96-b6b6-42ec-d182-5ea6dceecd49"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.28.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "lLtouVGZUd6n"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUON0nQnUjGI",
        "outputId": "4d2f216d-32d6-4aab-f465-5763c9802545"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Load the pretrained SciBERT model and tokenizer\n",
        "model_name = 'allenai/specter'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Generate embeddings for each abstract\n",
        "embeddings = []\n",
        "for i in tqdm(range(len(new_dataset))):\n",
        "    abstract = new_dataset[i]['abstract']\n",
        "    # Tokenize the abstract\n",
        "    tokens = tokenizer.tokenize(abstract)\n",
        "    # Remove stop words\n",
        "    tokens_without_stopwords = [token for token in tokens if token.lower() not in stop_words]\n",
        "    # Encode the abstract\n",
        "    encoded_abstract = tokenizer.encode_plus(' '.join(tokens_without_stopwords),\n",
        "                                             add_special_tokens=True,\n",
        "                                             max_length=512,\n",
        "                                             truncation=True,\n",
        "                                             padding='max_length',\n",
        "                                             return_tensors='pt').to(device)\n",
        "    # Generate SciBERT embedding for the abstract\n",
        "    with torch.no_grad():\n",
        "        outputs = model(encoded_abstract.input_ids)\n",
        "        embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()  # Extract embedding from the first token [CLS]\n",
        "    embeddings.append(embedding)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0ghKIkgUnoQ",
        "outputId": "9ea3470a-4c21-48fa-8764-3b26087bae36"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26048/26048 [16:11<00:00, 26.82it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Convert the embeddings list to a numpy array and reshape it\n",
        "embeddings_array = np.array(embeddings).reshape(len(embeddings), -1)"
      ],
      "metadata": {
        "id": "XTAyBYEiUyZv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len_pap = len(dataset['candidates'])"
      ],
      "metadata": {
        "id": "-t6_wLYbU2KZ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recommended_papers = []\n",
        "for i in tqdm(range(len_pap)):\n",
        "  query = dataset['query'][i]\n",
        "\n",
        "  # Preprocess the query in the same way as the documents\n",
        "  query_tokens = tokenizer.tokenize(query)\n",
        "  query_tokens_without_stopwords = [token for token in query_tokens if token.lower() not in stop_words]\n",
        "  encoded_query = tokenizer.encode_plus(' '.join(query_tokens_without_stopwords),\n",
        "                                        add_special_tokens=True,\n",
        "                                        max_length=512,\n",
        "                                        truncation=True,\n",
        "                                        padding='max_length',\n",
        "                                        return_tensors='pt')\n",
        "\n",
        "  # Generate SciBERT embedding for the query\n",
        "  with torch.no_grad():\n",
        "      query_input_ids = encoded_query.input_ids.to(device)\n",
        "      query_embedding = model(query_input_ids).last_hidden_state[:, 0, :].cpu().numpy()\n",
        "\n",
        "  # Calculate cosine similarity between the query embedding and document embeddings\n",
        "  query_embedding_tensor = torch.tensor(query_embedding).to(device)\n",
        "  similarity_scores = cosine_similarity(query_embedding_tensor.cpu().numpy(), embeddings_array)\n",
        "  # print(len(query_embedding_tensor.cpu().numpy()))\n",
        "  # print(len(embeddings_array))\n",
        "  # break;\n",
        "\n",
        "  # Get the indices of the top 10 similar documents\n",
        "  top_indices = np.argsort(similarity_scores, axis=1)[:, -10000:][0][::-1]\n",
        "\n",
        "  # Retrieve the IDs of the top similar documents\n",
        "  top_documents = [new_dataset[i]['id'] for i in top_indices]\n",
        "  temp = []\n",
        "  for doc_id in top_documents:\n",
        "    temp.append(doc_id)\n",
        "  recommended_papers.append(temp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7twtrxhiU2tE",
        "outputId": "2c53af37-8458-484d-b98b-a5ae1046f1a8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2637/2637 [07:22<00:00,  5.96it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def calculate_dcg(scores):\n",
        "    positions = np.arange(1, len(scores) + 1)\n",
        "    discounts = np.log2(positions + 1)\n",
        "    return np.sum(scores / discounts)\n",
        "\n",
        "def calculate_ndcg(ground_truth, recommended_documents, k):\n",
        "    ndcg_scores = []\n",
        "    for gt_docs, rec_docs in zip(ground_truth, recommended_documents):\n",
        "        relevance_scores = np.zeros(k)\n",
        "        for i, doc_id in enumerate(rec_docs[:k]):\n",
        "            if doc_id in gt_docs:\n",
        "                relevance_scores[i] = 1 # Assign a relevance score of 1 for relevant documents\n",
        "        ideal_dcg = calculate_dcg(sorted(relevance_scores, reverse=True))\n",
        "        dcg = calculate_dcg(relevance_scores)\n",
        "        ndcg = dcg / ideal_dcg if ideal_dcg > 0 else 0.5  # Handle division by zero\n",
        "        ndcg_scores.append(ndcg)\n",
        "    return np.mean(ndcg_scores)\n",
        "\n",
        "\n",
        "k = 5  # Consider top k recommended documents for NDCG calculation\n",
        "\n",
        "ndcg = calculate_ndcg(ground_truth, recommended_papers, k)\n",
        "print(f\"NDCG@{k}: {ndcg:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9Uvmm7zVB3J",
        "outputId": "6fe5cb45-d599-41f3-a8e5-940f77ac08d5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NDCG@5: 0.5345\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(embeddings_array[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-Ol8B_HiVxl",
        "outputId": "d93cefe4-dcb8-4d5f-dca7-412c41d73d91"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(query_embedding_tensor.cpu().numpy()[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhKxINMTimfB",
        "outputId": "47679e1e-b779-4de6-c664-91f42351b4f3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_dC5PiLeiw2T"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}